{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from random import shuffle, sample, choice\n",
    "import numpy as np\n",
    "from scipy.signal import convolve\n",
    "from scipy.io import loadmat\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from librosa import load, resample, get_samplerate, get_duration\n",
    "import soundfile as sf\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_dir = Path('/home/gmvwasse/MS-SNSD/clean_train')\n",
    "noise_dir = Path('/home/gmvwasse/MS-SNSD/noise_train')\n",
    "RIR_dir = Path('/project/RIRs')\n",
    "\n",
    "SNRs = [-10, -5, 0, 5, 10, 20, 30, 40]\n",
    "\n",
    "validation_frac = 0.1\n",
    "training_frac = 1 - validation_frac\n",
    "\n",
    "max_size = 37800\n",
    "\n",
    "output_dir = Path('/project/EHNet/WAVs/dataset_reverb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "def SNR(s, n):\n",
    "    return 10*math.log10(power(s)/power(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_convolve(speech_wav, RIR_path, SNRs):\n",
    "    while True:\n",
    "        try:\n",
    "            RIR = loadmat(RIR_path)\n",
    "        except IOError:\n",
    "            continue\n",
    "        break\n",
    "    RIR = RIR['h_air'][0]\n",
    "    speech_wav_upsampled = resample(speech_wav, 16000, 48000, res_type='kaiser_fast')\n",
    "    convolved = convolve(speech_wav_upsampled, RIR)\n",
    "    convolved = convolved[:3*len(speech_wav)]\n",
    "    convolved_downsampled = resample(convolved, 48000, 16000, res_type='kaiser_fast')\n",
    "    current_SNR = SNR(speech_wav, convolved_downsampled)\n",
    "    alpha = 1/math.sqrt(math.pow(10, (choice(SNRs) - current_SNR)/10))\n",
    "    return speech_wav + alpha * convolved_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_mixer(clean, noise, snr):\n",
    "    # Normalizing to -25 dB FS\n",
    "    rmsclean = (clean**2).mean()**0.5\n",
    "    scalarclean = 10 ** (-25 / 20) / rmsclean\n",
    "    clean = clean * scalarclean\n",
    "    rmsclean = (clean**2).mean()**0.5\n",
    "\n",
    "    rmsnoise = (noise**2).mean()**0.5\n",
    "    scalarnoise = 10 ** (-25 / 20) /rmsnoise\n",
    "    noise = noise * scalarnoise\n",
    "    rmsnoise = (noise**2).mean()**0.5\n",
    "    \n",
    "    # Set the noise level for a given SNR\n",
    "    noisescalar = np.sqrt(rmsclean / (10**(snr/10)) / rmsnoise)\n",
    "    noisenewlevel = noise * noisescalar\n",
    "    noisyspeech = clean + noisenewlevel\n",
    "    return noisyspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wavs(speech_dir, noise_dir, RIR_dir, speech_filenames, noise_filenames, SNRs, output_path, filecounter):\n",
    "    clean_output_path = output_path.joinpath('clean')\n",
    "    noisy_output_path = output_path.joinpath('noisy')\n",
    "        \n",
    "    RIR_filenames = os.listdir(RIR_dir)\n",
    "\n",
    "    while True:\n",
    "        ##------- Getting speech -------##\n",
    "        global clean_counter, noise_counter\n",
    "        idx_s_counter, idx_n_counter = clean_counter, noise_counter\n",
    "        speech_wav = []\n",
    "        # make sure all WAVs are 128000 samples (8s) long\n",
    "        length_difference = 128000 - len(speech_wav)\n",
    "        while length_difference > 0:\n",
    "            with idx_s_counter.get_lock():\n",
    "                idx_s_counter.value += 1\n",
    "                idx_s = idx_s_counter.value % np.size(speech_filenames)\n",
    "\n",
    "            path_speech = speech_dir.joinpath(speech_filenames[idx_s])\n",
    "            while True:\n",
    "                try:\n",
    "                    if get_samplerate(path_speech) == 16000:\n",
    "                        new_speech_wav, _ = load(path_speech, sr=None, mono=True)\n",
    "                    else:\n",
    "                        new_speech_wav, _ = load(path_speech, sr=16000, mono=True, res_type='kaiser_fast')\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            cleanconcat = np.append(speech_wav, np.zeros(int(16000*0.2))) # 0.2 seconds of silence between utterances\n",
    "            speech_wav = np.append(cleanconcat, new_speech_wav)\n",
    "\n",
    "            length_difference = 128000 - len(speech_wav)\n",
    "\n",
    "        if length_difference == 0:\n",
    "            pass\n",
    "        if length_difference < 0:\n",
    "            start = np.random.randint(0, -length_difference)\n",
    "            speech_wav = speech_wav[start:start+128000]\n",
    "\n",
    "        assert len(speech_wav) == 128000, 'Speech waveform has ' + str(len(speech_wav)) + ' samples instead of 128000 samples!'\n",
    "\n",
    "        RIR_filenames = [filename for filename in RIR_filenames if filename.endswith('.mat')]\n",
    "        RIR_path = RIR_dir.joinpath(choice(RIR_filenames))\n",
    "        speech_convolved_wav = resample_convolve(speech_wav, RIR_path, [20, 30, 40])\n",
    "\n",
    "        # normalize speech part to -25dBFS\n",
    "        rms = (speech_convolved_wav ** 2).mean() ** 0.5\n",
    "        scalar = 10 ** (-25 / 20) / (rms)\n",
    "        speech_convolved_wav = speech_convolved_wav * scalar\n",
    "        speech_wav = speech_wav * scalar\n",
    "\n",
    "\n",
    "        ##------- Adding noise -------##\n",
    "        with idx_n_counter.get_lock():\n",
    "            idx_n_counter.value += 1\n",
    "            idx_n = idx_n_counter.value % np.size(noise_filenames)\n",
    "        path_noise = noise_dir.joinpath(noise_filenames[idx_n])\n",
    "        while True:\n",
    "            try:\n",
    "                if get_samplerate(path_noise) == 16000:\n",
    "                    noise_wav, _ = load(path_noise, sr=None, mono=True)\n",
    "                else:\n",
    "                    noise_wav, _ = load(path_noise, sr=16000, mono=True, res_type='kaiser_fast')\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # normalize noise part to -25dBFS\n",
    "        rms = (noise_wav ** 2).mean() ** 0.5\n",
    "        scalar = 10 ** (-25 / 20) / (rms)\n",
    "        noise_wav = noise_wav * scalar\n",
    "\n",
    "        length_difference = len(speech_wav) - len(noise_wav)\n",
    "        if length_difference == 0:\n",
    "            pass\n",
    "        if length_difference > 0:\n",
    "            print(\"Noise was shorter than 8s!\")\n",
    "            continue\n",
    "        if length_difference < 0:\n",
    "            start = np.random.randint(0, -length_difference)\n",
    "            noise_wav = noise_wav[start:start+len(speech_wav)]\n",
    "\n",
    "        desired_SNR = choice(SNRs)\n",
    "        noisy_speech_wav = snr_mixer(speech_convolved_wav, noise_wav, desired_SNR)\n",
    "\n",
    "        if np.max(np.abs(noisy_speech_wav)) > 1: # skip if we are clipping\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                clean_speech_filename = clean_output_path.joinpath(str(filecounter) + '.wav')\n",
    "                sf.write(clean_speech_filename, speech_wav, 16000)\n",
    "\n",
    "                noisy_speech_filename = noisy_output_path.joinpath(str(filecounter) + '+SNR' + str(desired_SNR) + 'dB' + '.wav')\n",
    "                sf.write(noisy_speech_filename, noisy_speech_wav, 16000)\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(args1, args2):\n",
    "    global clean_counter, noise_counter\n",
    "    clean_counter = args1\n",
    "    noise_counter = args2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_generate_wavs(speech_dir, noise_dir, RIR_dir, SNRs, validation_frac, training_frac, max_size, output_dir):\n",
    "    speech_filenames = os.listdir(speech_dir)\n",
    "    speech_filenames = [filename for filename in speech_filenames if filename.endswith('.wav')]\n",
    "    shuffle(speech_filenames)\n",
    "    \n",
    "    noise_filenames = os.listdir(noise_dir)\n",
    "    noise_filenames = [filename for filename in noise_filenames if filename.endswith('.wav') and get_duration(filename=noise_dir.joinpath(filename)) >= 8]\n",
    "    shuffle(noise_filenames)\n",
    "    \n",
    "    print('Using', len(speech_filenames), 'speech files.')\n",
    "    print('Using', len(noise_filenames), 'noise files.')\n",
    "    \n",
    "\n",
    "    speech_train, speech_val = np.split(speech_filenames, [int(training_frac*len(speech_filenames))])\n",
    "    \n",
    "    training_output_path = output_dir.joinpath('training')\n",
    "    val_output_path = output_dir.joinpath('validation')\n",
    "    \n",
    "    global clean_counter, noise_counter\n",
    "    clean_counter = mp.Value('i', 0)\n",
    "    noise_counter = mp.Value('i', 0)    \n",
    "    \n",
    "    # generate training set\n",
    "    clean_output_path = training_output_path.joinpath('clean')\n",
    "    noisy_output_path = training_output_path.joinpath('noisy')\n",
    "    if not Path.exists(clean_output_path):\n",
    "        os.makedirs(clean_output_path)\n",
    "    if not Path.exists(noisy_output_path):\n",
    "        os.makedirs(noisy_output_path)\n",
    "    params = [speech_dir, noise_dir, RIR_dir, speech_train, noise_filenames, SNRs, training_output_path]\n",
    "    params = [repeat(param) for param in params]\n",
    "    with mp.Pool(processes=4, initializer=init, initargs=(clean_counter, noise_counter, )) as multi_pool:\n",
    "        multi_pool.starmap(generate_wavs, zip(*params, range(int(max_size * training_frac))))\n",
    "    \n",
    "    # generate validation set\n",
    "    clean_output_path = val_output_path.joinpath('clean')\n",
    "    noisy_output_path = val_output_path.joinpath('noisy')\n",
    "    if not Path.exists(clean_output_path):\n",
    "        os.makedirs(clean_output_path)\n",
    "    if not Path.exists(noisy_output_path):\n",
    "        os.makedirs(noisy_output_path)\n",
    "    params = [speech_dir, noise_dir, RIR_dir, speech_val, noise_filenames, SNRs, val_output_path]\n",
    "    params = [repeat(param) for param in params]\n",
    "    with mp.Pool(processes=4, initializer=init, initargs=(clean_counter, noise_counter, )) as multi_pool:\n",
    "        multi_pool.starmap(generate_wavs, zip(*params, range(int(max_size * validation_frac))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 23075 speech files.\n",
      "Using 128 noise files.\n"
     ]
    }
   ],
   "source": [
    "split_generate_wavs(speech_dir, noise_dir, RIR_dir, SNRs, validation_frac, training_frac, max_size, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
